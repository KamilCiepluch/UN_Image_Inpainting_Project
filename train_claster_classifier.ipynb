{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_VAUkJmgEdX"
   },
   "source": [
    "# Klasyfikator klastrÃ³w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laxkoQ58ggfA"
   },
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8W9ttzHf99W",
    "outputId": "5693aedd-35e6-4ee5-a49b-28c13dd816ce"
   },
   "source": [
    "!pip install comet-ml"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e45-0lqzgj9j",
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:20.664825Z",
     "start_time": "2025-01-22T00:59:20.662447Z"
    }
   },
   "source": [
    "import comet_ml\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "45PCWipEgnBU",
    "ExecuteTime": {
     "end_time": "2025-01-15T01:28:12.368328Z",
     "start_time": "2025-01-15T01:28:12.359126Z"
    }
   },
   "source": [
    "# import os\n",
    "\n",
    "# from google.colab import userdata\n",
    "# key = userdata.get('COMET_API_KEY')\n",
    "# os.environ['COMET_API_KEY'] = key"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWG9YAAkgoJc",
    "outputId": "8f44db06-3666-462a-aeef-06af544ec0ba",
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:26.361201Z",
     "start_time": "2025-01-22T00:59:26.341911Z"
    }
   },
   "source": [
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # Set default tensor type for cuda\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        # Ensure we're using float32 on CPU\n",
    "        torch.set_default_dtype(torch.float64)\n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "print(f\"Using {device} device\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s2k90WCgqQY"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_PJ_NkxgrlL",
    "outputId": "ae8347dc-f12b-4dc8-87d3-adddb446c5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5vSo6Jsrgs9s",
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:31.347088Z",
     "start_time": "2025-01-22T00:59:30.287199Z"
    }
   },
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qv7sMSzHjQjJ",
    "outputId": "64ffdebe-8aa1-439f-9d4c-7fc140e25336",
    "ExecuteTime": {
     "end_time": "2025-01-15T01:09:01.105210Z",
     "start_time": "2025-01-15T01:09:01.085363Z"
    }
   },
   "source": [
    "!ls"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "uNu2rfoPgugO",
    "outputId": "1862995f-20a0-46c3-b79c-fe8d77f16ab8",
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:32.504378Z",
     "start_time": "2025-01-22T00:59:32.173252Z"
    }
   },
   "source": "ds = load_from_disk(\"clustered_dataset\").with_format(\"torch\")",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vON2G2Cxg2H_",
    "ExecuteTime": {
     "end_time": "2025-01-15T01:28:56.204781Z",
     "start_time": "2025-01-15T01:28:56.200667Z"
    }
   },
   "source": [
    "ds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
       "        num_rows: 82600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
       "        num_rows: 10325\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
       "        num_rows: 10325\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nAkyAQSKg6Sk",
    "ExecuteTime": {
     "end_time": "2025-01-15T01:28:21.074853Z",
     "start_time": "2025-01-15T01:28:21.065769Z"
    }
   },
   "source": [
    "percent10 = True\n",
    "\n",
    "if percent10:\n",
    "  # Extract 10% of the train set\n",
    "  ten_percent_train = ds[\"train\"].select(range(int(len(ds[\"train\"]) * 0.1)))\n",
    "  # Extract 10% of the test set\n",
    "  ten_percent_test = ds[\"test\"].select(range(int(len(ds[\"test\"]) * 0.1)))\n",
    "  # Extract 10% of the validation set\n",
    "  ten_percent_valid = ds[\"valid\"].select(range(int(len(ds[\"valid\"]) * 0.1)))\n",
    "\n",
    "  # Combine the subsets into a new DatasetDict\n",
    "  ten_percent_dataset = DatasetDict({\n",
    "      \"train\": ten_percent_train,\n",
    "      \"test\": ten_percent_test,\n",
    "      \"valid\": ten_percent_valid\n",
    "  })\n",
    "\n",
    "  ds = ten_percent_dataset"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:37.345298Z",
     "start_time": "2025-01-22T00:59:37.341397Z"
    }
   },
   "source": [
    "  # change if not colab\n",
    "  num_workers = 12\n",
    "  pin_memory = True\n",
    "  batch_size = 64\n",
    "  train_loader = DataLoader(\n",
    "      ds[\"train\"],\n",
    "      batch_size=batch_size,\n",
    "      num_workers=num_workers,\n",
    "      shuffle=True,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  test_loader = DataLoader(\n",
    "      ds[\"test\"],\n",
    "      batch_size=batch_size,\n",
    "      num_workers=num_workers,\n",
    "      shuffle=False,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  val_loader = DataLoader(\n",
    "      ds[\"test\"],\n",
    "      batch_size=batch_size,\n",
    "      num_workers=num_workers,\n",
    "      shuffle=False,\n",
    "      pin_memory=True,\n",
    "  )"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_eSbu9vjozd"
   },
   "source": [
    "## ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fsIS8DP0imCW",
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:45.855735Z",
     "start_time": "2025-01-22T00:59:45.517806Z"
    }
   },
   "source": [
    "PATH = \"models/inpating/mse_perceptual/model1.pth\"\n",
    "# model = VGG16Autoencoder()\n",
    "new_model = torch.load(PATH)\n",
    "new_model.eval()\n",
    "new_model.to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamil\\AppData\\Local\\Temp\\ipykernel_5892\\1824298818.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_model = torch.load(PATH)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG16Autoencoder(\n",
       "  (encoder): VGG16EncoderWithSkipConnections(\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block6): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): VGG16Decoder(\n",
       "    (block6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block5): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (5): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:51.262910Z",
     "start_time": "2025-01-22T00:59:51.257666Z"
    }
   },
   "source": [
    "encoder = new_model.encoder\n",
    "encoder.eval()  # Set the encoder to evaluation mode\n",
    "encoder.to(device)  # Move the encoder to the appropriate device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16EncoderWithSkipConnections(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block6): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:59:54.046781Z",
     "start_time": "2025-01-22T00:59:54.043270Z"
    }
   },
   "source": [
    "# Define the MLP model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdaptedStyleClusterCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten the latent output [batch_size, 128, 4, 4] -> [batch_size, 2048]\n",
    "            \n",
    "            # Fully Connected Layers\n",
    "            nn.Linear(in_features=2048, out_features=128),  # Adjusted in_features\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T01:00:59.852886Z",
     "start_time": "2025-01-22T01:00:59.840658Z"
    }
   },
   "source": [
    "# Set up model hyperparameters\n",
    "input_size = 2048  \n",
    "num_classes = 20  \n",
    "\n",
    "# Create the model\n",
    "model = AdaptedStyleClusterCNN(num_classes)\n",
    "model.load_state_dict(torch.load(\"models/cluster_clasification/model1_20\"))\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamil\\AppData\\Local\\Temp\\ipykernel_5892\\1010332387.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"models/cluster_clasification/model1_20\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaptedStyleClusterCNN(\n",
       "  (model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T01:01:30.270887Z",
     "start_time": "2025-01-22T01:01:28.959088Z"
    }
   },
   "source": [
    "# pure PyTorch loop\n",
    "num_epochs = 200\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchinfo import summary\n",
    "from tqdm import tqdm \n",
    "from create_mask import generate_scaled_blob\n",
    "\n",
    "# SET UP COMET ML\n",
    "comet_experiment = comet_ml.Experiment(\n",
    "    api_key=\"LP4wJZSrJYL1KJZ06ahrmPLUb\",\n",
    "    project_name=\"UczenieNienadzorowane\")\n",
    "comet_experiment.log_code(folder=\"/UN\")\n",
    "comet_experiment.log_parameters(\n",
    "    {\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"train_size\": ds[\"train\"].num_rows,\n",
    "        \"val_size\": ds[\"valid\"].num_rows,\n",
    "    }\n",
    ")\n",
    "input_size = (batch_size, 128, 4, 4)\n",
    "summ = summary(model, input_size, device=device, depth=5)\n",
    "comet_experiment.set_model_graph(f\"{model.__repr__()}\\n{summ}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T13:20:48.105396Z",
     "start_time": "2025-01-22T01:01:58.767631Z"
    }
   },
   "source": [
    "# train and validate\n",
    "save_model_path = \"models/cluster_clasification/model1_0\"\n",
    "for epoch in range(num_epochs):\n",
    "        comet_experiment.set_epoch(epoch)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "        model.train()\n",
    "        with comet_experiment.train() as train:\n",
    "            for idx, batch in tqdm(enumerate(train_loader), desc=f\"TRAIN_{epoch}\"):\n",
    "                comet_experiment.set_step(idx + epoch * len(train_loader))\n",
    "\n",
    "                optimizer.zero_grad()  # MUST be called on every batch\n",
    "                \n",
    "                images = batch[\"image\"] / 255.0\n",
    "                labels = batch[\"cluster\"]\n",
    "\n",
    "                # Generate and apply mask\n",
    "                masks = generate_scaled_blob(images.shape, mask_percentage=(1 / 16) * 100).float() / 255.0\n",
    "                images_with_mask = images * (1 - masks.unsqueeze(1))\n",
    "                images_with_mask = torch.cat((images_with_mask, masks.unsqueeze(1)), dim=1)\n",
    "\n",
    "                images = images_with_mask.to(device)\n",
    "                \n",
    "                # One-hot encode labels\n",
    "                labels = F.one_hot(labels, num_classes=20).float().to(device)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "                latents, _ = encoder(images)\n",
    "                outputs = model(latents)  # Shape: [batch_size, num_classes=20]\n",
    "\n",
    "                loss = loss_func(outputs, labels)  # BCEWithLogitsLoss expects one-hot encoded labels\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                comet_experiment.log_metric(\"loss\", loss.item())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            save_model_path = f\"models/cluster_clasification/model1_{epoch+20}\"\n",
    "            \n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        with comet_experiment.validate() as validat, torch.no_grad() as nograd:\n",
    "            for idx, batch in tqdm(enumerate(val_loader), desc=f\"VAL_{epoch}\"):\n",
    "                comet_experiment.set_step(idx + epoch * len(val_loader))\n",
    "\n",
    "                images = batch[\"image\"] / 255.0\n",
    "                labels = batch[\"cluster\"]\n",
    "\n",
    "                # Generate and apply mask\n",
    "                masks = generate_scaled_blob(images.shape, mask_percentage=(1 / 16) * 100).float() / 255.0\n",
    "                images_with_mask = images * (1 - masks.unsqueeze(1))\n",
    "                images_with_mask = torch.cat((images_with_mask, masks.unsqueeze(1)), dim=1)\n",
    "\n",
    "                images = images_with_mask.to(device)\n",
    "                \n",
    "                # One-hot encode labels\n",
    "                labels = F.one_hot(labels, num_classes=20).float().to(device)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "                latents, _ = encoder(images)\n",
    "                outputs = model(latents)  # Shape: [batch_size, num_classes=20]\n",
    "\n",
    "                loss = loss_func(outputs, labels)  # BCEWithLogitsLoss expects one-hot encoded labels\n",
    "                comet_experiment.log_metric(\"loss\", loss.item())\n",
    "                "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN_0: 3it [00:08,  1.95s/it]C:\\Users\\kamil\\.conda\\envs\\DLF\\Lib\\site-packages\\scipy\\interpolate\\_fitpack_py.py:155: RuntimeWarning: The maximal number of iterations (20) allowed for finding smoothing\n",
      "spline with fp=s has been reached. Probable cause: s too small.\n",
      "(abs(fp-s)/s>0.001)\n",
      "  res = _impl.splprep(x, w, u, ub, ue, k, task, s, t, full_output, nest, per,\n",
      "TRAIN_0: 1291it [08:32,  2.52it/s]\n",
      "VAL_0: 162it [00:34,  4.69it/s]\n",
      "TRAIN_1: 1291it [08:14,  2.61it/s]\n",
      "VAL_1: 162it [00:33,  4.90it/s]\n",
      "TRAIN_2: 1291it [08:14,  2.61it/s]\n",
      "VAL_2: 162it [00:33,  4.89it/s]\n",
      "TRAIN_3: 1291it [08:17,  2.59it/s]\n",
      "VAL_3: 162it [00:35,  4.59it/s]\n",
      "TRAIN_4: 1291it [08:27,  2.54it/s]\n",
      "VAL_4: 162it [00:34,  4.65it/s]\n",
      "TRAIN_5: 1291it [08:16,  2.60it/s]\n",
      "VAL_5: 162it [00:33,  4.84it/s]\n",
      "TRAIN_6: 1291it [08:16,  2.60it/s]\n",
      "VAL_6: 162it [00:33,  4.81it/s]\n",
      "TRAIN_7: 1291it [08:18,  2.59it/s]\n",
      "VAL_7: 162it [00:34,  4.72it/s]\n",
      "TRAIN_8: 1291it [08:23,  2.57it/s]\n",
      "VAL_8: 162it [00:34,  4.75it/s]\n",
      "TRAIN_9: 1291it [08:22,  2.57it/s]\n",
      "VAL_9: 162it [00:34,  4.68it/s]\n",
      "TRAIN_10: 1291it [08:27,  2.54it/s]\n",
      "VAL_10: 162it [00:39,  4.09it/s]\n",
      "TRAIN_11: 1291it [08:34,  2.51it/s]\n",
      "VAL_11: 162it [00:35,  4.50it/s]\n",
      "TRAIN_12: 1291it [08:27,  2.54it/s]\n",
      "VAL_12: 162it [00:34,  4.66it/s]\n",
      "TRAIN_13: 1291it [08:27,  2.55it/s]\n",
      "VAL_13: 162it [00:34,  4.63it/s]\n",
      "TRAIN_14: 1291it [08:26,  2.55it/s]\n",
      "VAL_14: 162it [00:34,  4.67it/s]\n",
      "TRAIN_15: 1291it [08:27,  2.54it/s]\n",
      "VAL_15: 162it [00:34,  4.69it/s]\n",
      "TRAIN_16: 1291it [08:25,  2.55it/s]\n",
      "VAL_16: 162it [00:34,  4.68it/s]\n",
      "TRAIN_17: 1291it [08:30,  2.53it/s]\n",
      "VAL_17: 162it [00:39,  4.08it/s]\n",
      "TRAIN_18: 1291it [08:34,  2.51it/s]\n",
      "VAL_18: 162it [00:36,  4.46it/s]\n",
      "TRAIN_19: 1291it [08:26,  2.55it/s]\n",
      "VAL_19: 162it [00:35,  4.62it/s]\n",
      "TRAIN_20: 1291it [08:28,  2.54it/s]\n",
      "VAL_20: 162it [00:35,  4.61it/s]\n",
      "TRAIN_21: 1291it [08:26,  2.55it/s]\n",
      "VAL_21: 162it [00:35,  4.60it/s]\n",
      "TRAIN_22: 1291it [08:27,  2.54it/s]\n",
      "VAL_22: 162it [00:34,  4.64it/s]\n",
      "TRAIN_23: 1291it [08:27,  2.54it/s]\n",
      "VAL_23: 162it [00:34,  4.67it/s]\n",
      "TRAIN_24: 1291it [08:31,  2.53it/s]\n",
      "VAL_24: 162it [00:39,  4.05it/s]\n",
      "TRAIN_25: 1291it [08:34,  2.51it/s]\n",
      "VAL_25: 162it [00:36,  4.44it/s]\n",
      "TRAIN_26: 1291it [08:27,  2.54it/s]\n",
      "VAL_26: 162it [00:34,  4.66it/s]\n",
      "TRAIN_27: 1291it [08:28,  2.54it/s]\n",
      "VAL_27: 162it [00:35,  4.52it/s]\n",
      "TRAIN_28: 1291it [08:39,  2.48it/s]\n",
      "VAL_28: 162it [00:36,  4.44it/s]\n",
      "TRAIN_29: 1291it [08:27,  2.54it/s]\n",
      "VAL_29: 162it [00:35,  4.60it/s]\n",
      "TRAIN_30: 1291it [08:31,  2.52it/s]\n",
      "VAL_30: 162it [00:35,  4.60it/s]\n",
      "TRAIN_31: 1291it [08:34,  2.51it/s]\n",
      "VAL_31: 162it [00:40,  4.05it/s]\n",
      "TRAIN_32: 1291it [08:31,  2.52it/s]\n",
      "VAL_32: 162it [00:35,  4.60it/s]\n",
      "TRAIN_33: 1291it [08:33,  2.51it/s]\n",
      "VAL_33: 162it [00:35,  4.61it/s]\n",
      "TRAIN_34: 1291it [08:30,  2.53it/s]\n",
      "VAL_34: 162it [00:35,  4.57it/s]\n",
      "TRAIN_35: 1291it [08:29,  2.53it/s]\n",
      "VAL_35: 162it [00:34,  4.64it/s]\n",
      "TRAIN_36: 1291it [08:29,  2.53it/s]\n",
      "VAL_36: 162it [00:35,  4.63it/s]\n",
      "TRAIN_37: 1291it [08:29,  2.54it/s]\n",
      "VAL_37: 162it [00:35,  4.62it/s]\n",
      "TRAIN_38: 1291it [08:37,  2.49it/s]\n",
      "VAL_38: 162it [00:39,  4.10it/s]\n",
      "TRAIN_39: 1291it [08:38,  2.49it/s]\n",
      "VAL_39: 162it [00:36,  4.41it/s]\n",
      "TRAIN_40: 1291it [08:30,  2.53it/s]\n",
      "VAL_40: 162it [00:35,  4.62it/s]\n",
      "TRAIN_41: 1291it [08:30,  2.53it/s]\n",
      "VAL_41: 162it [00:34,  4.64it/s]\n",
      "TRAIN_42: 1291it [08:29,  2.54it/s]\n",
      "VAL_42: 162it [00:34,  4.66it/s]\n",
      "TRAIN_43: 1291it [08:29,  2.54it/s]\n",
      "VAL_43: 162it [00:35,  4.62it/s]\n",
      "TRAIN_44: 1291it [08:30,  2.53it/s]\n",
      "VAL_44: 162it [00:36,  4.42it/s]\n",
      "TRAIN_45: 1291it [08:32,  2.52it/s]\n",
      "VAL_45: 162it [00:38,  4.16it/s]\n",
      "TRAIN_46: 1291it [08:40,  2.48it/s]\n",
      "VAL_46: 162it [00:36,  4.39it/s]\n",
      "TRAIN_47: 1291it [08:29,  2.53it/s]\n",
      "VAL_47: 162it [00:39,  4.15it/s]\n",
      "TRAIN_48: 1291it [08:38,  2.49it/s]\n",
      "VAL_48: 162it [00:36,  4.44it/s]\n",
      "TRAIN_49: 1291it [08:30,  2.53it/s]\n",
      "VAL_49: 162it [00:35,  4.62it/s]\n",
      "TRAIN_50: 1291it [08:29,  2.53it/s]\n",
      "VAL_50: 162it [00:35,  4.61it/s]\n",
      "TRAIN_51: 1291it [08:33,  2.51it/s]\n",
      "VAL_51: 162it [00:39,  4.06it/s]\n",
      "TRAIN_52: 1291it [08:38,  2.49it/s]\n",
      "VAL_52: 162it [00:35,  4.61it/s]\n",
      "TRAIN_53: 1291it [08:31,  2.52it/s]\n",
      "VAL_53: 162it [00:34,  4.64it/s]\n",
      "TRAIN_54: 1291it [08:29,  2.53it/s]\n",
      "VAL_54: 162it [00:34,  4.65it/s]\n",
      "TRAIN_55: 1291it [08:30,  2.53it/s]\n",
      "VAL_55: 162it [00:34,  4.64it/s]\n",
      "TRAIN_56: 1291it [08:30,  2.53it/s]\n",
      "VAL_56: 162it [00:34,  4.65it/s]\n",
      "TRAIN_57: 1291it [08:29,  2.53it/s]\n",
      "VAL_57: 162it [00:35,  4.60it/s]\n",
      "TRAIN_58: 1291it [08:34,  2.51it/s]\n",
      "VAL_58: 162it [00:40,  4.03it/s]\n",
      "TRAIN_59: 1291it [08:40,  2.48it/s]\n",
      "VAL_59: 162it [00:36,  4.42it/s]\n",
      "TRAIN_60: 1291it [08:31,  2.52it/s]\n",
      "VAL_60: 162it [00:34,  4.64it/s]\n",
      "TRAIN_61: 1291it [08:31,  2.52it/s]\n",
      "VAL_61: 162it [00:34,  4.64it/s]\n",
      "TRAIN_62: 1291it [08:29,  2.53it/s]\n",
      "VAL_62: 162it [00:34,  4.68it/s]\n",
      "TRAIN_63: 1291it [08:30,  2.53it/s]\n",
      "VAL_63: 162it [00:34,  4.66it/s]\n",
      "TRAIN_64: 1291it [08:39,  2.49it/s]\n",
      "VAL_64: 162it [00:38,  4.18it/s]\n",
      "TRAIN_65: 1291it [08:38,  2.49it/s]\n",
      "VAL_65: 162it [00:38,  4.18it/s]\n",
      "TRAIN_66: 1291it [08:28,  2.54it/s]\n",
      "VAL_66: 162it [00:34,  4.67it/s]\n",
      "TRAIN_67: 1291it [08:22,  2.57it/s]\n",
      "VAL_67: 162it [00:37,  4.30it/s]\n",
      "TRAIN_68: 1291it [08:24,  2.56it/s]\n",
      "VAL_68: 162it [00:33,  4.79it/s]\n",
      "TRAIN_69: 1291it [08:21,  2.57it/s]\n",
      "VAL_69: 162it [00:33,  4.83it/s]\n",
      "TRAIN_70: 1291it [08:18,  2.59it/s]\n",
      "VAL_70: 162it [00:33,  4.83it/s]\n",
      "TRAIN_71: 1291it [08:20,  2.58it/s]\n",
      "VAL_71: 162it [00:33,  4.83it/s]\n",
      "TRAIN_72: 1291it [08:26,  2.55it/s]\n",
      "VAL_72: 162it [00:38,  4.23it/s]\n",
      "TRAIN_73: 1291it [08:26,  2.55it/s]\n",
      "VAL_73: 162it [00:34,  4.66it/s]\n",
      "TRAIN_74: 1291it [08:22,  2.57it/s]\n",
      "VAL_74: 162it [00:33,  4.81it/s]\n",
      "TRAIN_75: 1291it [08:20,  2.58it/s]\n",
      "VAL_75: 162it [00:33,  4.85it/s]\n",
      "TRAIN_76: 1291it [08:20,  2.58it/s]\n",
      "VAL_76: 162it [00:33,  4.86it/s]\n",
      "TRAIN_77: 1291it [08:19,  2.59it/s]\n",
      "VAL_77: 162it [00:33,  4.84it/s]\n",
      "TRAIN_78: 1291it [08:19,  2.59it/s]\n",
      "VAL_78: 162it [00:33,  4.81it/s]\n",
      "TRAIN_79: 1291it [09:52,  2.18it/s]\n",
      "VAL_79: 162it [00:35,  4.58it/s]\n",
      "TRAIN_80: 1291it [09:02,  2.38it/s]\n",
      "VAL_80: 162it [00:40,  3.99it/s]\n",
      "TRAIN_81: 29it [00:16,  1.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 35\u001B[0m\n\u001B[0;32m     33\u001B[0m         loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     34\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 35\u001B[0m         comet_experiment\u001B[38;5;241m.\u001B[39mlog_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     38\u001B[0m     save_model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/cluster_clasification/model1_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m20\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:28:33.045308Z",
     "start_time": "2025-01-15T04:27:58.189139Z"
    }
   },
   "source": [
    "model.eval()\n",
    "with comet_experiment.test() as test, torch.no_grad():\n",
    "    for idx, batch in tqdm(enumerate(test_loader), desc=f\"TEST_{num_epochs}\"):\n",
    "        comet_experiment.set_step(idx + num_epochs * len(test_loader))\n",
    "\n",
    "        images = batch[\"image\"] / 255.0\n",
    "        labels = batch[\"cluster\"]\n",
    "\n",
    "        # Generate and apply mask\n",
    "        masks = generate_scaled_blob(images.shape, mask_percentage=(1 / 16) * 100).float() / 255.0\n",
    "        images_with_mask = images * (1 - masks.unsqueeze(1))\n",
    "        images_with_mask = torch.cat((images_with_mask, masks.unsqueeze(1)), dim=1)\n",
    "\n",
    "        images = images_with_mask.to(device)\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        labels = F.one_hot(labels, num_classes=20).float().to(device)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        latents, _ = encoder(images)\n",
    "        outputs = model(latents)  # Shape: [batch_size, num_classes=20]\n",
    "\n",
    "        loss = loss_func(outputs, labels)  # BCEWithLogitsLoss expects one-hot encoded labels\n",
    "        \n",
    "        comet_experiment.log_metric(\"loss\", loss.item())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST_20: 162it [00:34,  4.74it/s]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:28:33.226625Z",
     "start_time": "2025-01-15T04:28:33.219600Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), f\"models/cluster_clasification/model1_{num_epochs}\")",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "comet_experiment.end()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
